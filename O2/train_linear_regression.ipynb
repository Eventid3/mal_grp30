{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "import numpy as np\n",
    "\n",
    "class MyLinReg(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self: 'MyLinReg', eta0=0.01, max_iter=10000, tol=1e-4, n_iter_no_change=10, verbose=True):\n",
    "        self.eta0: float = eta0\n",
    "        self.max_iter: int = max_iter\n",
    "        self.tol: float = tol\n",
    "        self.n_iter_no_change: int = n_iter_no_change\n",
    "        self.verbose: bool = verbose\n",
    "        self.intercept_: float = 0.0 # just a dummy init value\n",
    "        self.coef_: np.ndarray = np.array([0.0]) # just a dummy init value\n",
    "\n",
    "    def _MSE(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        assert len(y_pred) == len(y_true) and y_pred.ndim == 1 and y_true.ndim == 1\n",
    "        err_vec = y_pred - y_true # Xw - y\n",
    "        return np.mean(err_vec ** 2)\n",
    "\n",
    "    # loss function isnt used. But could be used in terms of early stopping.\n",
    "    def _loss(self: 'MyLinReg', X, y) -> float:\n",
    "        y_pred = self.predict(X)\n",
    "        return self._MSE(y_pred, y)\n",
    "\n",
    "    def __str__(self: 'MyLinReg') -> str: \n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "\n",
    "    def fit(self: 'MyLinReg', X: np.ndarray, y: np.ndarray) -> None:        \n",
    "        # Run batch or stocastich\n",
    "        self._batch(X, y)\n",
    "        #self._sgd(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self: 'MyLinReg', X: np.ndarray) -> np.ndarray:\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "    \n",
    "    def _batch(self: 'MyLinReg', X: np.ndarray, y: np.ndarray):\n",
    "        # Add bias term\n",
    "        X_b = add_dummy_feature(X)\n",
    "\n",
    "        n_samples, n_features = X_b.shape\n",
    "        np.random.seed(42)\n",
    "        theta = np.random.randn(n_features, 1) # random initialization of parameters\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            # Compute predictions: X w\n",
    "            y_pred = X_b @ theta\n",
    "\n",
    "            # Compute gradient: (2/n) X_b^T (y_pred - y)\n",
    "            gradient = (2.0 / n_samples) * X_b.T @ (y_pred - y.reshape(-1, 1))\n",
    "            \n",
    "            # Update theta\n",
    "            theta -= self.eta0 * gradient\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.intercept_ = float(theta.ravel()[0])\n",
    "        self.coef_ = theta[1:].ravel()\n",
    "                \n",
    "    # its kinda funny how \"unpredictable\" SGD is. You can really finetune with all the hyperparameters(and you have to)\n",
    "    def _sgd(self: 'MyLinReg', X: np.ndarray, y: np.ndarray):\n",
    "        # Add bias term\n",
    "        X_b = add_dummy_feature(X)\n",
    "\n",
    "        n_samples, n_features = X_b.shape\n",
    "        np.random.seed(42)\n",
    "        theta = np.random.randn(n_features, 1) # random initialization of parameters\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            # Shuffle data, this means there is no \"tilbagel√¶gning\" during an epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            # This part is almost identical to HOML, but without the learning schedule(variable eta)\n",
    "            for i in indices:\n",
    "                xi = X_b[i:i+1]\n",
    "                yi = y[i:i+1]\n",
    "\n",
    "                # Prediction\n",
    "                y_pred_i = xi @ theta\n",
    "\n",
    "                # Compute gradient: 2 X_b^T (y_pred - y)\n",
    "                gradient = 2.0 * xi.T @ (y_pred_i - yi.reshape(-1, 1))\n",
    "\n",
    "                # Update theta\n",
    "                theta -= self.eta0 * gradient\n",
    "\n",
    "        # Store final parameters\n",
    "        self.intercept_ = float(theta.ravel()[0])\n",
    "        self.coef_ = theta[1:].ravel()\n",
    "        \n",
    "    #def score(self, X, y_true): # inherited from RegressorMixin\n",
    "        #assert False, \"TODO: implement me, or inherit me\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue}\n",
    "$$\n",
    "\n",
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "#### Epoch vs iteration\n",
    "\n",
    "One epoch of training is when the entire training set has been iterated. An iteration is when \n",
    "\n",
    "An iteration is an update on the weights.\n",
    "\n",
    "BGD: In our Batch Gradient Descent, epoch and iteration is essentially the same thing, since we update the weights after each pass on the training set.\n",
    "\n",
    "SGD: In our Batch Gradient Descent, each random access into the training set results in an iteration. In our training set, we shuffle and each sample, is only used once. This results in x iterations per epoch, where x is the number of samples in the training set.\n",
    "\n",
    "<href> https://medium.com/@sujathamudadla1213/epoch-vs-batch-vs-iteration-in-neural-networks-8d02ea155304\n",
    "\n",
    "#### Numerical vs closed form\n",
    "\n",
    "The numerical is the \"engineer way\" of doing things, with many iterations, you try to minimize a loss function. It may not find the optimal solution because of local and global optimums.\n",
    "\n",
    "The closed form it the \"math way\" of doing it. You solve the system using a lot of algebra and find a solution that in a single step will find the optimal weights. It may be very hard to find the closed form. It may also have requirements/assumptions for the sets, like the fact that a matrix must be invertible:\n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "For this we have chosen to stick with the IRIS smoke tester. Here are the results for the IRIS dataset:\n",
    "\n",
    "Batch:\n",
    "\n",
    "INFO:  SCORE['MyLinReg'] = 0.891\n",
    "\n",
    "INFO:  SCORE['SGDRegressor'] = 0.913\n",
    "\n",
    "SGD:\n",
    "\n",
    "INFO:  SCORE['MyLinReg'] = 0.945\n",
    "\n",
    "INFO:  SCORE['SGDRegressor'] = 0.913\n",
    "\n",
    "It seems that the linear regressor actually does a decent job.\n",
    "\n",
    "The MNIST dataset is just way too large to compute without variable learning rate, so skip that if running this notebook...\n",
    "\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "Note that this implementation uses type hints(variable: type). This allows for better IDE support.\n",
    "\n",
    "#### Constructor\n",
    "\n",
    "The linear regressor has a default constructor that takes optional parameters. Each of these is saved as member variables in the instantiated object.\n",
    "\n",
    "#### MSE and loss function\n",
    "\n",
    "The MSE and loss function are implemented as private functions. The MSE is based on the formula:\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$\n",
    "\n",
    "The loss function(though not used), calculates the predicted values based on theta, and then calls the MSE function.\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "If we were to use the loss function, it could be in terms of early stopping. If we found that several iterations gave no significant progress to the loss function, meaning it was platueing(perhaps its at a minimum), then we might as well stop iterating.\n",
    "\n",
    "#### Fit\n",
    "\n",
    "The fit method forwards the input to either the _batch or _sgd functions.\n",
    "\n",
    "The start of both methods adds the bias, using the add_dummy_feature(X) from sk_learn:\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "After that we instantiate a theta, being our weights(w). These are set to random values to avoid a stagnating model.\n",
    "\n",
    "Batch\n",
    "\n",
    "During our batch gradient descent, each iteration equals an epoch, which means we use the entire training set when updating weights. This is shown in the following equation:\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "SGD\n",
    "\n",
    "In each epoch, the stochastic gradient descent shuffles the entire training set. Then it iterates over it and plucks one out and performs an update on the weights. When the entire training set has been exhausted, a new epoch is started.\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= 2 \\left(\\mathbf{x}^{(i)}\\right)^\\top \\left( \\mathbf{x}^{(i)} \\bw - \\mathbf{y}^{(i)}_{\\textrm{true}} \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "#### Predict\n",
    "\n",
    "The predict function simply multiplies the stored coefficients to the input and adds the stored bias/intercept.\n",
    "This is the same for the Batch and Stocastich implementation.\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARN:  This mini smoke-test may produce false-positives and/or\n",
      "       false-negatives..\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  y_pred = [5.61498307 6.75547413 4.04730909 5.18372294]\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE = \u001b[1;34m0.4950056429557478\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  bias         = 4.046879011698211\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  coefficients = [1.88012149]\u001b[0m\n",
      "\n",
      "       w         =[4.05 1.88]\n",
      "       w_expected=[4.05 1.88]\n",
      "\n",
      "\u001b[1;35mINFO:  Well, good news, your w and the expected w-vector seem to be\n",
      "       very close numerically, so the smoke-test has passed!\u001b[0m\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor: TestMyLinReg\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "### SOME NIFTY HELPER FUNS ###\n",
    "\n",
    "def isVector(y, expected_n=-1):\n",
    "    assert isinstance(y, numpy.ndarray), f\"expected type 'numpy.array' but got {type(y)}\"\n",
    "    assert y.ndim==1, f\"expected y.ndim==1 but got {y.ndim}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[0], f\"expected vector of size {expected_n} but got size {y.shape}\"\n",
    "    return True\n",
    "\n",
    "def isMatrix(X, expected_m=-1, expected_n=-1):\n",
    "    assert isinstance(X, numpy.ndarray), f\"expected type 'numpy.array' but got {type(X)}\"\n",
    "    assert X.ndim==2, f\"expected X.ndim==2 but got {X.ndim}\"\n",
    "    assert expected_m<0 or expected_m==y.shape[0], f\"expected matrix of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[1], f\"expected vector of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    return True\n",
    "\n",
    "def PrintMatrix(x, label=\"\", precision=12, linewidth=60):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        # NOTE: how does multiple import behave, any performance issues?\n",
    "        from libitmal.utils import PrintMatrix as FancyPrintMatrix\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        FancyPrintMatrix(x, label=label, precision=precision, linewidth=linewidth)\n",
    "    else:\n",
    "        # default simple implementation\n",
    "        print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def Col(color):\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import Col as FancyCol\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyCol(color)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ColEnd():\n",
    "    hasFancy = False\n",
    "    try:\n",
    "        from libitmal.Utils.colors import ColEnd as FancyColEnd\n",
    "        hasFancy = True\n",
    "    except Exception as ex:\n",
    "        Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyColEnd()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "\n",
    "    def FormatTxt(txt, linewidth=60, prefix=\"\", replacetabs=True):\n",
    "        assert isinstance(txt, str)\n",
    "        assert isinstance(linewidth, int) and linewidth > 0\n",
    "        assert isinstance(prefix, str)\n",
    "\n",
    "        if replacetabs:\n",
    "            txt = txt.replace(\"\\t\",\"    \")\n",
    "\n",
    "        r = \"\"\n",
    "        n = 0\n",
    "        m = 0\n",
    "        for i in txt:\n",
    "            m += 1\n",
    "            if n >= linewidth:\n",
    "                if not i.isspace() and m < len(txt) and not txt[m].isspace():\n",
    "                    r += \"\\\\\" # add hypen\n",
    "                r += \"\\n\" + prefix\n",
    "                n = 0\n",
    "\n",
    "            if n == 0 and i.isspace():\n",
    "                continue # skip leading space\n",
    "\n",
    "            r += i\n",
    "            n += 1\n",
    "\n",
    "            if i == \"\\n\":\n",
    "                r += prefix\n",
    "                n = 0\n",
    "\n",
    "        return r\n",
    "\n",
    "    col_beg = Col(color)\n",
    "    col_end = ColEnd()\n",
    "\n",
    "    prefix = \"\".ljust(len(pre_msg)) \n",
    "    msg = FormatTxt(msg, prefix=prefix)\n",
    "    \n",
    "    print(f\"{col_beg}{pre_msg}{msg}{col_end}\\n\", file=filestream)\n",
    "\n",
    "    if ex is not None:\n",
    "        #msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "        PrintOutput(str(ex), prefix + \"EXCEPTION: \", None, \"red\", filestream)\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARN:  \", ex, \"lyellow\")\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"lred\" )\n",
    "    raise Exception(msg) if ex is None else ex\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"INFO:  \", None, \"lpurple\")\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'max_iter'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(eta0=0.01)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'eta0'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(verbose=False)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'verbose'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(tol=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'tol'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(n_iter_no_change=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'n_iter_no_change'\", ex)\n",
    "\n",
    "    # create regressor with default hyperparameter values\n",
    "    # to be used for training, prediction and score..\n",
    "    try:\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ with default parameters\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {Col('lblue')}{score}{ColEnd()}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef_\n",
    "            bias = regressor.intercept_\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import PrintMatrix\n",
    "    except Exception as ex:\n",
    "        PrintMatrix = SimplePrintMatrix # fall-back\n",
    "        Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            \n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            \n",
    "            PrintMatrix(w,          label=\"       w         =\")\n",
    "            PrintMatrix(w_expected, label=\"       w_expected=\")\n",
    "            print()\n",
    "            \n",
    "            eps = 1E-2 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps)\n",
    "            Info(\"Well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "            \n",
    "            return regressor\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "    \n",
    "    return None\n",
    "\n",
    "Warn(\"This mini smoke-test may produce false-positives and/or\\n false-negatives..\")\n",
    "TestMyLinReg()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mINFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['MyLinReg']..\u001b[0m\n",
      "\n",
      "y_pred_test=[ 1.6701  1.2746  1.1848  1.8069  0.132   1.8463  1.3976\n",
      "              0.9879 -0.0994 -0.1052  0.023  -0.0805 -0.037   0.0637\n",
      "              1.2917  1.0957  2.0711  2.0841  1.0807  1.3299  1.102\n",
      "              1.4883 -0.1859  1.5861  1.5035  0.0693  0.0467  2.0145\n",
      "              1.7379 -0.3361  1.7629  1.3306  1.169  -0.1319 -0.0811\n",
      "              1.2357  0.1313  2.0062]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['MyLinReg'] = \u001b[1;34m0.891\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['SGDRegressor']..\u001b[0m\n",
      "\n",
      "y_pred_test=[ 1.2138  1.407   1.2178  1.8232 -0.0234  1.8371  1.318\n",
      "              0.9827  0.0547 -0.0561 -0.062  -0.0485 -0.0388 -0.1151\n",
      "              1.3112  1.0564  2.079   1.8649  1.3098  1.1317  1.3167\n",
      "              1.61    0.1104  1.2416  1.2024  0.0113  0.0714  1.9479\n",
      "              2.1788 -0.0245  1.6796  1.1594  1.4916 -0.2205  0.0462\n",
      "              1.4527 -0.0512  1.5301]\n",
      "\n",
      "\u001b[1;35mINFO:  SCORE['SGDRegressor'] = \u001b[1;34m0.913\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  ##############################################\n",
      "       \u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  DATA: 'MNIST'\n",
      "       SHAPES: X_train=(52500, 784), X_test=(17500, 784), y_train=(\\\n",
      "       52500,), y_test=(17500,)\u001b[0m\n",
      "\n",
      "\u001b[1;35mINFO:  TRAINING['MyLinReg']..\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m         Info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m##############################################\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# somewhat more verbose testing, you regressor will likely fail on MNIST \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# or at least be very, very slow... It cannot train on MNIST XD\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m TestAndCompareRegressors()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mTestAndCompareRegressors\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTRAINING[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()), r])\n\u001b[1;32m---> 29\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     31\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     33\u001b[0m PrintMatrix(y_pred_test, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_test=\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manni\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manni\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m, in \u001b[0;36mMyLinReg.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyLinReg\u001b[39m\u001b[38;5;124m'\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray, y: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:        \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Run batch or stocastich\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch(X, y)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#self._sgd(X, y)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 50\u001b[0m, in \u001b[0;36mMyLinReg._batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m X_b \u001b[38;5;241m@\u001b[39m theta\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Compute gradient: (2/n) X_b^T (y_pred - y)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m gradient \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m n_samples) \u001b[38;5;241m*\u001b[39m X_b\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Update theta\u001b[39;00m\n\u001b[0;32m     53\u001b[0m theta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta0 \u001b[38;5;241m*\u001b[39m gradient\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow... It cannot train on MNIST XD\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
