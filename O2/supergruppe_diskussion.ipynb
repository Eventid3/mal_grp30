{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Supergruppe diskussion\n",
    "\n",
    "\n",
    "## § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel  § 2 og forbered mundtlig præsentation.\n",
    "\n",
    "## Forberedelse inden lektionen\n",
    "\n",
    "Een eller flere af gruppe medlemmer forbereder et mundtligt resume af § 2:\n",
    "\n",
    "* i skal kunne give et kort mundligt resume af hele § 2 til en anden gruppe (på nær, som nævnt, Create the Workspace og Download the Data),\n",
    "\n",
    "* resume holdes til koncept-plan, dvs. prøv at genfortælle, hvad de overordnede linier er i kapitlerne i [HOML].\n",
    "\n",
    "Lav et kort skriftlig resume af de enkelte underafsnit, ca. 5 til 20 liners tekst, se \"TODO\"-template herunder (MUST, til O2 aflevering).\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* _Look at the Big Picture,_\n",
    "* _Get the Data,_\n",
    "* _Explore and Visualize the Data to Gain Insights,_ \n",
    "* _Prepare the Data for Machine Learning Algorithms,_\n",
    "* _Select and Train a Model,_\n",
    "* _Fine-Tune Your Model,_\n",
    "* _Launch, Monitor, and Maintain Your System,_\n",
    "* (_Try It Out!._  er afslutning af kapitel, skal ikke resumeres).\n",
    "\n",
    "## På klassen\n",
    "\n",
    "Supergruppe [SG] resume af § 2 End-to-End, ca. 30 til 45 min.\n",
    "\n",
    "* en supergruppe [SG], sammensættes af to grupper [G], on-the-fly på klassen,\n",
    "\n",
    "* hver gruppe [G] forbereder og giver en anden gruppe [G] et mundtligt resume af § 2 til en anden gruppe,\n",
    "\n",
    "* tid: ca. 30 mim. sammenlagt, den ene grupper genfortæller første halvdel af § 2 i ca. 15 min., hvorefter den anden gruppe genfortæller resten i ca. 15 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "#### Resume: Get the Data\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "#### Resume: Explore and Visualize the Data to Gain Insights,\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "Of all the things you do when preparing data it is important to make this into single reusable functions, which can be used for retrainging the same model or new models in the future\n",
    "\n",
    "The first step is to clean your data. If an attribute has a lot of missing data you can either, remove the attribute or set the missing values to some value (zero, mean, median, etc.)\n",
    "Then there is the problem with text based attributed, since models usually like to train on numerical data. But if there is a set amount of text options, we're in luck because then they can be numerized. A initiate attempt at doing this, would be to make a sort of enum out of the text options. But this bears a problem, as most ML algorithms usually make the assumption that when numbers are close they resemble eachouter, which doesnt necessarly have to be the case with text options. Therefore the prefered method is to add a binary attribute for each text option, which is either 0 or 1.\n",
    "\n",
    "There is also the problem ff there is a big difference in the size of the numbers between attributes it can cause harm to trainign proccess of a model. E.g. pretty much all models would tend to ignore an attribute whose numbers range from 0-15 if another attribute range from 6-39.320. If this is the cause, it is a good idea to scale the values. One way to do this is with min-max scaling also known normalization where all values are scaled to be between 0-1. Another is standardization where all values are subtracted by the mean and divides it bny the standard deviation. \n",
    "\n",
    "Standardization handles outliers better than min-max scaling. To understand why its best to understand with an example. if the average value is 10, but there is an outlier which is 100, the 100 will unfairly squash all of the otherwise average values into the low range of the 0-1 interval (instead of having them be 0.5 as \"expected\"). Standardization is better at solving this \"squashing\"-problem. But if there a many outliers in one end (aka. a heavy tail), standardization will also fail. Here other methods are needed which differs from dataset to dataset.\n",
    "\n",
    "The chapther also touches on the subject of transformers which are tools used to do all of this data prepping. Another example given by the book is a transformer which is klustering to find clusters in the dataset and then creates new features for the ML algorithm to learn from\n",
    "\n",
    "\n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "The chapter covers 3 different models. The first is the classis linear model, which it trains on the house data. This gives out a poor perfomance with an RMSE of about 67000 thousand. Contextually this is a large number as house prices average on 120.000 - 265.000 dollars. \n",
    "Second model is the DecisionTreeRegressor, which has an RMSE of 0. It compltely overfits. Which means it learns the training data too good, and therefore perfoms badly on new test sets. This overfitting is found by using Scikit K-fold cross validation feature, which splits up datasets into n non-overlapping groups and then trains the model n times with a new group being the test set each time. \n",
    "Last model is the RandomForestRegressor, which perfoms better with a RMSE of 47.000 on a test set and 17.500 on the training set. So still som overfitting but still better. \n",
    "The chapther doesnt dive too deep into how the models work, but simply focus on the fact, that there are multiple models to choose from with different benefits.  \n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "\n",
    "This chapter covers methods to fine tune your choosen model\n",
    "\n",
    "Grid search:\n",
    "- Chose hyperparameters to finetune. Can be wherever in the pipeline/trainign process. Specify parameter and training values. if two parameters are chosen each which 3 test values 3 x 3 = 9 combinations are tested. Thereby the name \"grid search\"\n",
    "\n",
    "Randomized search:\n",
    "- Like grid search, but instead of searching all combinations, you set a max amount of iterations and the model simply pick out random combination. Apparently surprisingly good\n",
    "\n",
    "Analyzing:\n",
    "- Not all analyzing can be done by the computer. You, the programmer can also do a lot of manual fiddling. When the best model is found , if the reative importance of attributes can be found, its a good idea to remove parameters of low importance. Its good to clean outliers, look at the erros and try to figure out why it fails. Make subsets of your validation data to make sure it doesnt only work good on average. In the example of house pricing they make subsets of rural, urban, rich, poor, nothern, southern, minority or not.  \n",
    "\n",
    "Final testing:\n",
    "- 95 confidence interval is just a range, where you say \"I am 95 confident that the real average perfomance of my model is between these two numbers\". Its really good to have, because maybe your final test was either really luck or unlucky. Also usefull to test if your fine tuned model is actually better than your last one, if the fine tuned models scores are outside of the 95 confidence interval of the old model.\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "Model can be download with\n",
    "joblib.dump(final_model, \"my_california_housing_model.pkl\")\n",
    "Book recommends to make the model into its own web service aka. microservice, which you backend then can query to get predictions on data.\n",
    "It recommends Googles Vertex AI, which is a cloud service, which can automatically handle scaling and load balancing if your ML service gets a lot of traffic\n",
    "\n",
    "it highlights the importance of maintaining your model so it doesnt succumb to e.g. \"model rot\", where a model is trained on last year data, so it is thereby outdated in its prediction.\n",
    "It recommends to put in place a monitoring systen (with or without humans), and this whole mainting/monitoring process should be automated as much as possible as its usually a very big task - even bigger than training the model in the first place. 3 pointers are:\n",
    "1. Collect fresh data regularly and label it\n",
    "2. Write script to auto train train model an fine tune hyperparameters\n",
    "3. Write script which compares new model with old and automatically deploys new model if performance hasnt decreased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2019-01-28| CEF, initial.\n",
    "2020-02-05| CEF, F20 ITMAL update.\n",
    "2021-08-17| CEF, E21 ITMAL update.\n",
    "2021-09-17| CEF, corrected some spell errors.\n",
    "2022-01-28| CEF, update to F22 SWMAL.\n",
    "2022-09-09| CEF, corrected 'MUST for O1' to 'MUST for O2' in text.\n",
    "2023-02-13| CEF, updated to HOML 3rd, removed exclude subsections in 'Get the Data' in this excercise, since the parts with python environments has been removed in HOML.\n",
    "2024-02-14| CEF, removed resume of 'Try-it-out'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
