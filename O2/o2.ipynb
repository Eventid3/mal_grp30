{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97ae7ed",
   "metadata": {},
   "source": [
    "### Title: \"MAL 02\"\n",
    "\n",
    "| Gruppe 30 |\n",
    "| --------- |\n",
    "\n",
    "| Navn                    | Studienummer |\n",
    "| ----------------------- | ------------ |\n",
    "| Lasse Borring Petersen  | 202208165    |\n",
    "| Benjamin Harboe Strunge | 202209864    |\n",
    "| Esben Inglev            | 202210050    |\n",
    "| Asbj√∏rn Vad             | 202208512    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122ee13",
   "metadata": {},
   "source": [
    "# L3: Supergruppe diskussion\n",
    "\n",
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "This chapter is about everything you should do before you start working on the problem itself. You should start off by considering what kind of data you have, and what you want to find out from this data. Is it about categorizing the data? Or do you want your model to be able to predict an exact number instead. This all depends on what the model will be used for, and how it will fit into the overall system. Thereby this also affects the choice of machine learning algorithm that will be used, which is also important to get right early on, so you don't waste time working on something that is not needed.\n",
    "\n",
    "You should also consider what the current solution to the problem looks like, if there is one. This can give you and indication of how good your model needs to be, to be effective to the users. By understanding the current solution to the problem, it is easier to get to work on a new, better one.\n",
    "\n",
    "When all this is considered, you are ready to start designing the system. Now is the time to consider what kind of training supervision the model will be using. This can often be found out by understanding the problem, underlining the importance of the steps described above. Here you also select what kind of performance measure the model will be using. The book here describes a few possible options like RMSE and MAE, how they differ, and when they should each be used.\n",
    "\n",
    "Lastly, the book describes the importance of checking the assumptions that have been made to far, and that they are still correct. This can help catch errors earlier on, making the easier to deal with.\n",
    "\n",
    "#### Resume: Get the Data\n",
    "\n",
    "This chapter starts off with quite a long section about Google Colab, and how jupyter notebooks work, which we will skip here.\n",
    "\n",
    "Then we get to looking at the data structure, where we see the parameters, and the median house price. It is then described what kind of data we are looking at for each attribute, and how we can view it to understand it easier. It also explains the data snooping bias, and why it is important to make some decisions before looking at the data, and some after.\n",
    "\n",
    "After that it is described how and why we extract a test set, and how we make sure we dont mix up our test and training sets, across different runs. It also talks about stratas, which is a way of categorizing your data, and making sure your model has the correct amount of each category in each set.\n",
    "\n",
    "The chapter concludes by underlining the importance of test set generation, as this is the data that will define the model and its usefulness.\n",
    "\n",
    "#### Resume: Explore and Visualize the Data to Gain Insights,\n",
    "\n",
    "This chapter is about going more in depth about the data, exploring it and learning about its correlations.\n",
    "\n",
    "It starts off again making sure that you only work on the training data, and that you always have a copy of it, just in case.\n",
    "\n",
    "Then, because we are working with geographic data (Californian housing prices), we can viualize it with the longitude and latitude, clearly showing the state in the diagram. Then a few parameters are changed, to better the visualization, which ends up showing the most/least expensive areas, as well as the population density in these areas. This helps us identify the more expensive areas, which is obviously in the larger cities in the state.\n",
    "\n",
    "After that we look into the standard correlation coefficient between every pair of attributes. This describes the linear correlation between each attribute, going from -1, indicating a negative linear relationship (One goes up when the other goes down), to 1, indicating a positive linear relationship. here we see a quite strong correlation between the house value and the median income of said house, which is not surprising. This is then looked further into, plotting them against eachother. Here we can visually see the correlation between them, as well as the data cap on the house value of 500.000$.\n",
    "\n",
    "Lastly it talks about attribute combinations, where you can combine some attributes that are not that useful on their own (total number of rooms in district & number of households in district). Therefore 3 new attributes are created: Rooms per house, bedrooms ratio, and people per house. These are some attribues that might be more useful, as it is also seen when the book shows the correlation matrix again. Here we see that the bedrooms ratio has a negative correlation with the median house value, indicating that more expensive houses have more rooms in them that are not bedrooms.\n",
    "\n",
    "It concludes by mentioning that this is of course an iterative proces, where you can analyze your prototypes output, gain more insight into the data, and explore it further.\n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "Of all the things you do when preparing data it is important to make this into single reusable functions, which can be used for retrainging the same model or new models in the future\n",
    "\n",
    "The first step is to clean your data. If an attribute has a lot of missing data you can either, remove the attribute or set the missing values to some value (zero, mean, median, etc.)\n",
    "Then there is the problem with text based attributed, since models usually like to train on numerical data. But if there is a set amount of text options, we're in luck because then they can be numerized. A initiate attempt at doing this, would be to make a sort of enum out of the text options. But this bears a problem, as most ML algorithms usually make the assumption that when numbers are close they resemble eachouter, which doesnt necessarly have to be the case with text options. Therefore the prefered method is to add a binary attribute for each text option, which is either 0 or 1.\n",
    "\n",
    "There is also the problem ff there is a big difference in the size of the numbers between attributes it can cause harm to trainign proccess of a model. E.g. pretty much all models would tend to ignore an attribute whose numbers range from 0-15 if another attribute range from 6-39.320. If this is the cause, it is a good idea to scale the values. One way to do this is with min-max scaling also known normalization where all values are scaled to be between 0-1. Another is standardization where all values are subtracted by the mean and divides it bny the standard deviation.\n",
    "\n",
    "Standardization handles outliers better than min-max scaling. To understand why its best to understand with an example. if the average value is 10, but there is an outlier which is 100, the 100 will unfairly squash all of the otherwise average values into the low range of the 0-1 interval (instead of having them be 0.5 as \"expected\"). Standardization is better at solving this \"squashing\"-problem. But if there a many outliers in one end (aka. a heavy tail), standardization will also fail. Here other methods are needed which differs from dataset to dataset.\n",
    "\n",
    "The chapther also touches on the subject of transformers which are tools used to do all of this data prepping. Another example given by the book is a transformer which is klustering to find clusters in the dataset and then creates new features for the ML algorithm to learn from\n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "The chapter covers 3 different models. The first is the classis linear model, which it trains on the house data. This gives out a poor perfomance with an RMSE of about 67000 thousand. Contextually this is a large number as house prices average on 120.000 - 265.000 dollars.\n",
    "Second model is the DecisionTreeRegressor, which has an RMSE of 0. It compltely overfits. Which means it learns the training data too good, and therefore perfoms badly on new test sets. This overfitting is found by using Scikit K-fold cross validation feature, which splits up datasets into n non-overlapping groups and then trains the model n times with a new group being the test set each time.\n",
    "Last model is the RandomForestRegressor, which perfoms better with a RMSE of 47.000 on a test set and 17.500 on the training set. So still som overfitting but still better.\n",
    "The chapther doesnt dive too deep into how the models work, but simply focus on the fact, that there are multiple models to choose from with different benefits.\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "\n",
    "This chapter covers methods to fine tune your choosen model\n",
    "\n",
    "Grid search:\n",
    "\n",
    "- Chose hyperparameters to finetune. Can be wherever in the pipeline/trainign process. Specify parameter and training values. if two parameters are chosen each which 3 test values 3 x 3 = 9 combinations are tested. Thereby the name \"grid search\"\n",
    "\n",
    "Randomized search:\n",
    "\n",
    "- Like grid search, but instead of searching all combinations, you set a max amount of iterations and the model simply pick out random combination. Apparently surprisingly good\n",
    "\n",
    "Analyzing:\n",
    "\n",
    "- Not all analyzing can be done by the computer. You, the programmer can also do a lot of manual fiddling. When the best model is found , if the reative importance of attributes can be found, its a good idea to remove parameters of low importance. Its good to clean outliers, look at the erros and try to figure out why it fails. Make subsets of your validation data to make sure it doesnt only work good on average. In the example of house pricing they make subsets of rural, urban, rich, poor, nothern, southern, minority or not.\n",
    "\n",
    "Final testing:\n",
    "\n",
    "- 95 confidence interval is just a range, where you say \"I am 95 confident that the real average perfomance of my model is between these two numbers\". Its really good to have, because maybe your final test was either really luck or unlucky. Also usefull to test if your fine tuned model is actually better than your last one, if the fine tuned models scores are outside of the 95 confidence interval of the old model.\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "Model can be download with\n",
    "joblib.dump(final_model, \"my_california_housing_model.pkl\")\n",
    "Book recommends to make the model into its own web service aka. microservice, which you backend then can query to get predictions on data.\n",
    "It recommends Googles Vertex AI, which is a cloud service, which can automatically handle scaling and load balancing if your ML service gets a lot of traffic\n",
    "\n",
    "it highlights the importance of maintaining your model so it doesnt succumb to e.g. \"model rot\", where a model is trained on last year data, so it is thereby outdated in its prediction.\n",
    "It recommends to put in place a monitoring systen (with or without humans), and this whole mainting/monitoring process should be automated as much as possible as its usually a very big task - even bigger than training the model in the first place. 3 pointers are:\n",
    "\n",
    "1. Collect fresh data regularly and label it\n",
    "2. Write script to auto train train model an fine tune hyperparameters\n",
    "3. Write script which compares new model with old and automatically deploys new model if performance hasnt decreased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b423e",
   "metadata": {},
   "source": [
    "# L4: Dataanalyse til O4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99eb4ca",
   "metadata": {},
   "source": [
    "# L4: Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4a23f",
   "metadata": {},
   "source": [
    "# L5: Train linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477c7ea",
   "metadata": {},
   "source": [
    "# L6: ANN\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
