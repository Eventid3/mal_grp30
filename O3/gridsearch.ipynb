{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Qa Explain GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes\n",
    "There are a few additions made by the group.\n",
    "\n",
    "1. Ignoring warnings, since running with low max iterations would result in ConvergenceWarnings.\n",
    "2. SummarizeParamPerformance, which is used during random search to prune the possible hyperparameter candidates. When used later it will be explained.\n",
    "3. When running on the cluster the results are written to a file using the WriteReportToFile function.\n",
    "\n",
    "### Code review\n",
    "\n",
    "#### Cell 1\n",
    "The LoadAndSetupData function simply loads whichever dataset is desired and splits it into training and test sets.\n",
    "Then there are several functions that print out information using the grid_tune object. It prints the best parameters and scores based on those parameters.\n",
    "\n",
    "#### Cell 2\n",
    "First up the call to LoadAndSetupData is made to get the Iris dataset.\n",
    "Then the base model is chosen to be a Support Vector Classifier with the hyperparameter \"gamma\" set to 0.001.\n",
    "\n",
    "Then the hyperparameters that are to be searched are defined. Here, 2 different kernels, linear and rbf are selected. The C parameter should also be searched with values 0.1, 1 and 10. This means that 2\\*3 combinations are to be trained and scored.\n",
    "\n",
    "The GridSearch uses 5 cross validations. So you could say that it is 2\\*3\\*5 models that are to be trained and scored.\n",
    "\n",
    "The scoring is set to f1_micro, which takes all the results from the confusion matrix from all the different classes and computes a single F1 score from this. That means that no class is favoured.(https://datascience.stackexchange.com/questions/40900/whats-the-difference-between-sklearn-f1-score-micro-and-weighted-for-a-mult)\n",
    "\n",
    "n_jobs just means \"run on as many cpus/cores as possible, which will parallelize and speed up the grid search.\n",
    "\n",
    "The grid search implements the fit predict interface and is used first to fit and then in the FullReport which tells us about the best model.\n",
    "\n",
    "Apparently the best combination in our case is SVC(C=1, gamma=0.001, kernel='linear') yielding a score of 0.97143."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "# The following function was made by ChatGPT to display the average values of different parameters.\n",
    "# It should be usefull to quickly sort out any really bad parameters.\n",
    "def SummarizeParamPerformance(search, param='loss', sort=True):\n",
    "    results = search.cv_results_\n",
    "    params = np.array(results[f'param_{param}'], dtype=object)\n",
    "    scores = np.array(results['mean_test_score'])\n",
    "\n",
    "    unique_params = np.unique(params)\n",
    "    summary = []\n",
    "    for val in unique_params:\n",
    "        mask = params == val\n",
    "        mean_score = np.mean(scores[mask])\n",
    "        std_score = np.std(scores[mask])\n",
    "        summary.append((val, mean_score, std_score))\n",
    "\n",
    "    if sort:\n",
    "        summary.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nAverage CV score per '{param}':\")\n",
    "    for val, mean_score, std_score in summary:\n",
    "        print(f\"  {param}={val!s:15}  mean_f1_micro={mean_score:.4f}  (+/- {std_score:.4f})\")\n",
    "\n",
    "def WriteReportToFile(grid_tuned, X_test, y_test, t, params_to_summarize=None, \n",
    "                      full_results_file='search_results.txt', \n",
    "                      status_file='search_status.txt'):\n",
    "    \"\"\"\n",
    "    Captures FullReport and SummarizeParamPerformance output and writes to files.\n",
    "    \n",
    "    Args:\n",
    "        grid_tuned: The fitted GridSearchCV or RandomizedSearchCV object\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        t: Search time in seconds\n",
    "        params_to_summarize: List of parameter names to summarize (e.g., ['loss', 'penalty'])\n",
    "        full_results_file: Filename for complete results\n",
    "        status_file: Filename for status summary\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Capture all output to a string buffer\n",
    "    output_buffer = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = output_buffer\n",
    "    \n",
    "    b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "    \n",
    "    # Summarize parameters if provided\n",
    "    if params_to_summarize:\n",
    "        for param in params_to_summarize:\n",
    "            SummarizeParamPerformance(grid_tuned, param)\n",
    "    \n",
    "    print('OK(random-grid-search)')\n",
    "    print(b0)\n",
    "    \n",
    "    # Restore stdout\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    # Write full report to file\n",
    "    with open(full_results_file, 'w') as f:\n",
    "        f.write(output_buffer.getvalue())\n",
    "    \n",
    "    # Write just the OK and best model to a separate file\n",
    "    with open(status_file, 'w') as f:\n",
    "        f.write('OK(random-grid-search)\\n')\n",
    "        f.write(f'{b0}\\n')\n",
    "    \n",
    "    print(f\"Results written to {full_results_file} and {status_file}\")\n",
    "    \n",
    "    return b0, m0\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \"\"\"\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "    \n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "        \n",
    "    ok = kerasok or tensorflowkerasok\n",
    "    \n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "    \n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 6.30 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")  # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks,\n",
    "# FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "We set up a lot of hyperparameters and combinations to search through. There is no real structure other than the numerical values increasing by a factor 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: grid search\n",
    "# Setup data. Done above already\n",
    "#X_train, X_test, y_train, y_test = LoadAndSetupData('iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive', 'invscaling'],\n",
    "    'eta0': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [1000, 2500, 5000, 10000, 20000],\n",
    "    'shuffle': [True, False],\n",
    "    'average': [False, True],\n",
    "    'early_stopping': [False, True],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = WriteReportToFile(\n",
    "    grid_tuned, X_test, y_test, t,\n",
    "    params_to_summarize=None,\n",
    "    full_results_file='mnist_search_results_iris_cv.txt',\n",
    "    status_file='mnist_search_status_iris_cv.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "best: dat=iris, score=1.00000, model=SGDClassifier(alpha=0.01,average=False,early_stopping=False,eta0=1e-05,learning_rate='optimal',loss='modified_huber',max_iter=1000,penalty='l1',shuffle=False)\n",
    "\n",
    "SEARCH TIME: 122.54 sec\n",
    "\n",
    "Best model set found on train set:\n",
    "\n",
    "\tbest parameters={'alpha': 0.01, 'average': False, 'early_stopping': False, 'eta0': 1e-05, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': 'l1', 'shuffle': False}\n",
    "\tbest 'f1_micro' score=1.0\n",
    "\tbest index=28981\n",
    "\n",
    "Best estimator CTOR:\n",
    "\tSGDClassifier(alpha=0.01, eta0=1e-05, loss='modified_huber', penalty='l1',\n",
    "              shuffle=False)\n",
    "\n",
    "#### Comments on the run:\n",
    "We can see that the search 2 minutes, which isnt a long time, but the number of trained models is <=28981.\n",
    "If the dataset was bigger, training time would explode.\n",
    "\n",
    "The best score is 1, which should be concerning, but the dataset is so small so its alright in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "The n_iter parameter is the number of random combinations to test. We could have 4 loss functions, 3 penalty and 4 learning rates which would at least mean 4\\*3\\*4 = 48 models. However, if we set n-iter to 20, we are guaranteed to only get that amount of models.\n",
    "We can then quickly test a lot of random combinations and possibly rule out parameters that are not a good fit for the use case.\n",
    "\n",
    "Another advantage with the random search is that an interval can be specified rather than fixed values. There is the potential to get a lucky hit on a random number that just increases the scores dramatically(although it should be unlikely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: random grid search\n",
    "# Setup data. Done above already\n",
    "#X_train, X_test, y_train, y_test = LoadAndSetupData('iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "model = SGDClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive', 'invscaling'],\n",
    "    'alpha': loguniform(0.000001, 0.1),\n",
    "    'eta0': loguniform(0.00001, 0.1),\n",
    "    'max_iter': randint(1000, 20000),\n",
    "    'shuffle': [True, False],\n",
    "    'average': [False, True],\n",
    "    'early_stopping': [False, True],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run RandomizedSearchCV for the model\n",
    "grid_tuned = RandomizedSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          n_iter=100,\n",
    "                          random_state=42,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "\n",
    "SummarizeParamPerformance(grid_tuned)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "best: dat=iris, score=0.99048, model=SGDClassifier(alpha=0.024169519319206773,average=False,early_stopping=False,eta0=3.4360613844702604e-05,learning_rate='optimal',loss='modified_huber',max_iter=12470,penalty='l1',shuffle=True)\n",
    "\n",
    "SEARCH TIME: 2.90 sec\n",
    "\n",
    "Best model set found on train set:\n",
    "\n",
    "\tbest parameters={'alpha': 0.024169519319206773, 'average': False, 'early_stopping': False, 'eta0': 3.4360613844702604e-05, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 12470, 'penalty': 'l1', 'shuffle': True}\n",
    "\tbest 'f1_micro' score=0.9904761904761905\n",
    "\tbest index=92\n",
    "\n",
    "Best estimator CTOR:\n",
    "\tSGDClassifier(alpha=0.024169519319206773, eta0=3.4360613844702604e-05,\n",
    "              loss='modified_huber', max_iter=12470, penalty='l1')\n",
    "\n",
    "\n",
    "\tSGDClassifier(alpha=0.01, eta0=1e-05, loss='modified_huber', penalty='l1',\n",
    "              shuffle=False)\n",
    "\n",
    "#### Comments on the run\n",
    "The random search that tests 100 random models takes just 3 seconds to run, which is 1/60 of the time. The best model scores 0.99, which is also very high, but again, the dataset is very small. We see that they both find the same loss and penalty functions to be best. The differences are mostly at the alpha and eta0 values. With more models tested we might achieve close to the same values.\n",
    "\n",
    "One thing that is nice about the random search is that it scales linearly in time. So doubling n_iter, will take twice the time(disregarding the early stopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd MNIST Search Quest II\n",
    "This quest is in two parts. The first part is where we tried to use an SGD classifier and the second where we use the SVC.\n",
    "\n",
    "#### SGD\n",
    "Initially we thought using the SGD would be okay since it scored well with the IRIS dataset. We went through 3 different searches all using the random approach. After each run we looked at which parameters gave the worst scores and removed them from testing.\n",
    "\n",
    "For all the tests we scaled the data using the StandardScaler(we did not go back and try to remove it).\n",
    "\n",
    "The final result is 0.9207, which is posted to Brightspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "from time import time\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "# reference for why the loguniform is used.\n",
    "#https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization\n",
    "\n",
    "# Trying all the loss, penalty and learning_rate options\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive', 'invscaling'],\n",
    "    'alpha': loguniform(0.000001, 0.1),\n",
    "    'eta0': loguniform(0.00001, 0.1),\n",
    "    'max_iter': randint(1000, 20000),\n",
    "    'shuffle': [True, False],\n",
    "    'average': [False, True],\n",
    "    'early_stopping': [False, True],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = RandomizedSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    n_iter=200,              # will take a couple hours\n",
    "    random_state=42,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = WriteReportToFile(\n",
    "    grid_tuned, X_test, y_test, t,\n",
    "    params_to_summarize=['loss', 'penalty', 'learning_rate', 'shuffle', 'average', 'early_stopping'],\n",
    "    full_results_file='mnist_search_results.txt',\n",
    "    status_file='mnist_search_status.txt'\n",
    ")\n",
    "\n",
    "CV = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 1\n",
    "#### Input\n",
    "Trying all the loss, penalty and learning_rate options\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive', 'invscaling'],\n",
    "    'alpha': loguniform(0.000001, 0.1),\n",
    "    'eta0': loguniform(0.00001, 0.1),\n",
    "    'max_iter': randint(1000, 20000),\n",
    "    'shuffle': [True, False],\n",
    "    'average': [False, True],\n",
    "    'early_stopping': [False, True],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = RandomizedSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    n_iter=200,              # will take a couple hours\n",
    "    random_state=42,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#### Output\n",
    "SEARCH TIME: 9788.79 sec\n",
    "\n",
    "best: dat=mnist, score=0.91400, model=SGDClassifier(alpha=3.46979613767106e-05,average=True,early_stopping=True,eta0=0.0010845668034236337,learning_rate='optimal',loss='modified_huber',max_iter=18429,penalty='l2',shuffle=False)\n",
    "\n",
    "#### Parameter information\n",
    "Average CV score per 'loss':\n",
    "  loss=modified_huber   mean_f1_micro=0.8491  (+/- 0.1689)\n",
    "  loss=squared_hinge    mean_f1_micro=0.8143  (+/- 0.1880)\n",
    "  loss=hinge            mean_f1_micro=0.7767  (+/- 0.2730)\n",
    "  loss=log_loss         mean_f1_micro=0.7235  (+/- 0.3143)\n",
    "\n",
    "Average CV score per 'penalty':\n",
    "  penalty=l2               mean_f1_micro=0.8744  (+/- 0.0628)\n",
    "  penalty=elasticnet       mean_f1_micro=0.7799  (+/- 0.2627)\n",
    "  penalty=l1               mean_f1_micro=0.7362  (+/- 0.3033)\n",
    "\n",
    "Average CV score per 'learning_rate':\n",
    "  learning_rate=optimal          mean_f1_micro=0.8213  (+/- 0.2332)\n",
    "  learning_rate=invscaling       mean_f1_micro=0.7971  (+/- 0.1656)\n",
    "  learning_rate=constant         mean_f1_micro=0.7790  (+/- 0.2879)\n",
    "  learning_rate=adaptive         mean_f1_micro=0.7774  (+/- 0.2857)\n",
    "\n",
    "Average CV score per 'shuffle':\n",
    "  shuffle=False            mean_f1_micro=0.8069  (+/- 0.2175)\n",
    "  shuffle=True             mean_f1_micro=0.7765  (+/- 0.2759)\n",
    "\n",
    "Average CV score per 'average':\n",
    "  average=False            mean_f1_micro=0.8623  (+/- 0.0559)\n",
    "  average=True             mean_f1_micro=0.7240  (+/- 0.3292)\n",
    "\n",
    "Average CV score per 'early_stopping':\n",
    "  early_stopping=True             mean_f1_micro=0.8374  (+/- 0.1720)\n",
    "  early_stopping=False            mean_f1_micro=0.7413  (+/- 0.3031)\n",
    "\n",
    "This uses the created function mentioned in the beginning. We remove the parameters that score low and have a high variation, since they are unlikely to settle down.\n",
    "Then we start another round of searching. Please note that the search time is ~3 hours and yields a score of 0.914."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 2\n",
    "#### Input\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'modified_huber', 'squared_hinge'],\n",
    "    'penalty': ['l2', 'elasticnet'],\n",
    "    'learning_rate': ['adaptive', 'optimal'],\n",
    "    'alpha': loguniform(0.000001, 0.01),\n",
    "    'eta0': loguniform(0.0001, 0.1),\n",
    "    'max_iter': randint(5000, 60000),\n",
    "    'n_iter_no_change': [10],\n",
    "    'early_stopping': [True],\n",
    "    'shuffle': [True],\n",
    "    'average': [True, False],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = RandomizedSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    n_iter=200, # will take a couple hours\n",
    "    random_state=42,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "#### Output\n",
    "SEARCH TIME: 4805.65 sec\n",
    "\n",
    "best: dat=mnist, score=0.91937, model=SGDClassifier(alpha=0.0010316033434719725,average=False,early_stopping=True,eta0=0.013321207301714324,learning_rate='adaptive',loss='hinge',max_iter=57075,n_iter_no_change=10,penalty='elasticnet',shuffle=True,validation_fraction=0.1)\n",
    "\n",
    "#### Parameter information\n",
    "Average CV score per 'loss':\n",
    "  loss=squared_hinge    mean_f1_micro=0.8953  (+/- 0.0304)\n",
    "  loss=modified_huber   mean_f1_micro=0.8801  (+/- 0.1539)\n",
    "  loss=hinge            mean_f1_micro=0.8690  (+/- 0.1768)\n",
    "\n",
    "Average CV score per 'penalty':\n",
    "  penalty=l2               mean_f1_micro=0.9033  (+/- 0.0195)\n",
    "  penalty=elasticnet       mean_f1_micro=0.8571  (+/- 0.1960)\n",
    "\n",
    "Average CV score per 'learning_rate':\n",
    "  learning_rate=optimal          mean_f1_micro=0.8976  (+/- 0.0854)\n",
    "  learning_rate=adaptive         mean_f1_micro=0.8601  (+/- 0.1838)\n",
    "\n",
    "Average CV score per 'shuffle':\n",
    "  shuffle=True             mean_f1_micro=0.8807  (+/- 0.1398)\n",
    "\n",
    "Average CV score per 'average':\n",
    "  average=False            mean_f1_micro=0.9029  (+/- 0.0205)\n",
    "  average=True             mean_f1_micro=0.8626  (+/- 0.1857)\n",
    "\n",
    "Average CV score per 'early_stopping':\n",
    "  early_stopping=True             mean_f1_micro=0.8807  (+/- 0.1398)\n",
    "\n",
    "#### Discussion\n",
    "We now see that the squared_hinge loss function has the highest average score but with a low variation. In fact, the best model has the lowest average hinge loss function and the lower average elasticnet. This tells us that you can just get a really lucky combination that achieves a higher score. However the modified_huber got neither the highest score or the best model, so we drop that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 3\n",
    "#### Input\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'squared_hinge'],     # Dropping modified_huber\n",
    "    'penalty': ['l2', 'elasticnet'],\n",
    "    'learning_rate': ['optimal', 'adaptive'],\n",
    "    'alpha': loguniform(0.00001, 0.005),        # narrowing search around the best parameters found earlier\n",
    "    'eta0': loguniform(0.001, 0.05),         # narrowing search around the best parameters found earlier\n",
    "    'max_iter': randint(10000, 80000),      # increasing maximum iterations\n",
    "    'n_iter_no_change': [10],\n",
    "    'early_stopping': [True],\n",
    "    'shuffle': [True],\n",
    "    'average': [False],\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = RandomizedSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    n_iter=200, # will take some hours\n",
    "    random_state=42,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#### Output\n",
    "SEARCH TIME: 10949.07 sec\n",
    "\n",
    "best: dat=mnist, score=0.92073, model=SGDClassifier(alpha=0.0034952182314214584,average=False,early_stopping=True,eta0=0.030463008520733255,learning_rate='adaptive',loss='hinge',max_iter=75450,n_iter_no_change=10,penalty='l2',shuffle=True)\n",
    "\n",
    "#### Parameter information\n",
    "Average CV score per 'loss':\n",
    "  loss=hinge            mean_f1_micro=0.9115  (+/- 0.0049)\n",
    "  loss=squared_hinge    mean_f1_micro=0.8978  (+/- 0.0118)\n",
    "\n",
    "Average CV score per 'penalty':\n",
    "  penalty=l2               mean_f1_micro=0.9050  (+/- 0.0113)\n",
    "  penalty=elasticnet       mean_f1_micro=0.9032  (+/- 0.0116)\n",
    "\n",
    "Average CV score per 'learning_rate':\n",
    "  learning_rate=optimal          mean_f1_micro=0.9084  (+/- 0.0041)\n",
    "  learning_rate=adaptive         mean_f1_micro=0.9002  (+/- 0.0144)\n",
    "\n",
    "Average CV score per 'shuffle':\n",
    "  shuffle=True             mean_f1_micro=0.9041  (+/- 0.0115)\n",
    "\n",
    "Average CV score per 'average':\n",
    "  average=False            mean_f1_micro=0.9041  (+/- 0.0115)\n",
    "\n",
    "Average CV score per 'early_stopping':\n",
    "  early_stopping=True             mean_f1_micro=0.9041  (+/- 0.0115)\n",
    "\n",
    "\n",
    "#### Discussion\n",
    "We decided to stop searching using the SGD classifier since we barely got an increase in score. Looking at the variance there isnt much room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC\n",
    "Now we try the SVC which has fewer parameters than the SGD. The most important however is the kernel. We chose the rbf kernel since it is the best at multi-class classification when looking throught he scikit learn documentation. https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#sphx-glr-auto-examples-svm-plot-svm-kernels-py\n",
    "\n",
    "We do NOT scale the data. We found that scaling yielded a worse score than not scaling at all.\n",
    "Due to this late revelation there are not that many runs.\n",
    "\n",
    "The best score is 0.96949(might increase) and will be used as a last resort.\n",
    "\n",
    "The approach was similar to the SGD, but search time is way longer since it scales horribly with bigger datasets and thus fewer combinations could be tested per run.\n",
    "\n",
    "Initially we just used the \"auto\" and \"scale\" gamma parameters, to avoid having too many variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "tuning_parameters = {\n",
    "    'C': [4]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = GridSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = WriteReportToFile(\n",
    "    grid_tuned, X_test, y_test, t,\n",
    "    params_to_summarize=[\"C\"],\n",
    "    full_results_file='mnist_search_POC_cv.txt',\n",
    "    status_file='mnist_search_status_POC_cv.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "SEARCH TIME: 362.07 sec\n",
    "best: dat=mnist, score=0.98210, model=SVC(C=4)\n",
    "\n",
    "#### Comments\n",
    "Compared to many runs where the data was scaled and couldnt get above ~96.94, this was a clear improvement and a wider search is needed.\n",
    "This was just proof of concept, that the data shouldnt be scaled(at least not with the standardscaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "tuning_parameters = {\n",
    "    'C': [1, 2, 4, 8, 12, 16, 25, 35, 50],\n",
    "    'gamma': ['auto', 'scale'], # using the defined options since they are more, well, defined\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "grid_tuned = GridSearchCV(\n",
    "    model,\n",
    "    tuning_parameters,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "\n",
    "b0, m0 = WriteReportToFile(\n",
    "    grid_tuned, X_test, y_test, t,\n",
    "    params_to_summarize=[\"C\", \"gamma\"],\n",
    "    full_results_file='mnist_search_long_cv.txt',\n",
    "    status_file='mnist_search_status_long_cv.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "We hope to have a good output\n",
    "\n",
    "#### Comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "The benefits of grid search are clear:\n",
    "1. Using random grid search we can quickly track down a set of suitable parameters that could yield a good model.\n",
    "2. Then with the grid search we can do a more systematic search and remove even more parameters.\n",
    "3. Set the processor(CPU or GPU) to just crunch for hours on end without having to manually replace parameters.\n",
    "\n",
    "Some drawbacks/preconditions:\n",
    "1. There are no indications when a search is done, we can only estimate based on earlier runs. If the model scales badly, becomes even worse(SVC).\n",
    "2. You still need to have some idea what the parameters do to get a good search result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
